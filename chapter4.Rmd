---
title: "chapter 4"
author: "BB"
date: "24 november 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#exploration of Boston data
library(MASS)
data(Boston)
str(Boston)
dim(Boston)
```
Boston data has 506 observations of 14 variables about housing values in the suburbs of Boston, and contains info on for example average number of rooms per dwelling, weighted mean of distances to five Boston employment centres and an index of accessibility to radial highways.

```{r}
#Access corrplot and dplyr libraries
library(corrplot)
library(dplyr)

#create correlation matrix plots and a data summary
cor_matrix <- cor(Boston) %>% round(digits = 2)
corrplot(cor_matrix, method="circle", type = "upper", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
summary(Boston)
```
##Correlation plot
This correlation plot gives us a nice overview of the relations between the variables. The darker blue and larger the circle, the higher the positive correlation coefficient. The darker red and larger the circle, the stronger the negative correlation. The average distance to five Boston employment centres is negatively associated with age, nitrogen oxide concentration and proportion of non-retail business acres per town. Unsurprisingly, the lower status of the population is negatively linked to median value of owner-occupied homes in \$1000s.
There is a very strong positive correlation between accessibility to radial highways and full-value property-tax rate per \$10,000. Also, proportion of non-retail business acres per town is associated with nitrogen oxides concentration. 

##Summary
Through summary we get an indication of the distribution of the variable values. The minimal and maximal values are distinct for per capita crime rate by town, proportion of residential land, proportion of non-retail business acres per town, and the Charles River dummy variable, but the majority of the values are on the lower end (as visible through the low values of the 3rd quartile). The opposite is true for black; 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town. Most other variables seem to be fairly normally distributed.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summar scaled variables
summary(boston_scaled)

# check class of the boston_scaled 
class(boston_scaled)

# change class to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

Through scaling the means are subtracted from the actual values and subsequently divided by the standard deviation. This is an attempt to approach normal distribution and equal variance of the variables, which are requirements for linear discriminant analysis.

```{r}
# create a quantile vector of of scaled crim (crime) and print it
crim_quantile <- quantile(boston_scaled$crim)
crim_quantile

# create a categorical variable 'crime' and label categories from low to high
crime <- cut(boston_scaled$crim, breaks = crim_quantile, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# remove original crim from dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add  new categorical variable crime to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# call 'n' the number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

```

Here I created a categorical variable for per capita crime rate by town, with categories low, medium low, medium high and high. Also, I removed the old crime variable from the boston_scaled dataset.  Subsequently we create a train and a test variables with respectively a random 80% and 20% of the rows in boston_scaled.


```{r}
# linear discriminant analysis (lda)
lda.fit <- lda(crime ~ ., data = train)

# print lda.fit 
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

First I performed the linear discriminant analysis with crime as a target variable and all other variables as predictors. Then I prepared and performed the biplot. The index of accessibility to radial highways (rad) has a large variation, and has a positive impact on LD1, linked with high crime rates.

```{r}
# save the correct crime classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

First I saved the correct crime classes from the test set, and then I removed the crime variable from the test dataset. With the LDA model I try to predict the the crime classes. When cross-tabulating, We can see that most of the predictions are correct, as seen by the diagonal (overlapping) numbers, where the prediction and the correct model agree. All incorrectly predicted categories only vary one category from the correct category, so medium_low, instead of low. It never occurs that the prediction is low, and the correct answer would've been med_high or high.  



```{r}
#Reload Boston dataset (is opening library necessary??  )
library(MASS)
data(Boston)

# center and standardize variables
boston_scaled <- scale(Boston)

# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)
```

After reloading the Boston data and scaling it, I calculated the Euclidean distance of the variables. This gives the distance to the closest source for each variable. The median euclidian distance between variables is 171, and varies approximately between 1 and 626. 

```{r}
# k-means clustering
km <-kmeans(Boston, centers = 3)

library(ggplot2)
set.seed(123)

# determine the maximum number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```

First I perform a kmeans for 3 clusters. In this way variables are grouped in three clusters based on their distance between, or similarity to, each other. To find the optimal amount of clusters one can perform within cluster sum of squares (WCSS). In this way we find the number of clusters, where the observations are closest to the cluster centre. The optimal number of clusters will be decided on when the total value of WCSS changes radically. 

In the qplot one can see that this happens at the second cluster. There is a strong decrease in value from the first. The rest of the clusters vary among a similar value. Therefore, we decide to use 2 clusters for the kmeans. The function set.seed is used to prevent kmeans to produce different results every time, since it randomly assigns cluster centers. 

When looking at the Boston dataset divided by the two clusters, it seems like there are two separate crime groups, divided by no crime (red) versus any amount of crime (black). Findings were similar for accessibility to radial highways (low (red) versus high (black)) and property-tax rate.